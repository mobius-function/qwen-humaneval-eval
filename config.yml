# HumanEval Evaluation Configuration
# This file defines all experiment configurations for running inference and evaluation

# vLLM Server Settings
vllm:
  api_url: "http://localhost:8000/v1"
  max_tokens: 768  # Increased from 512 to allow more complex implementations
  top_p: 0.95
  stop_sequences:
    - "\nclass "
    - "\ndef "
    # REMOVED "\n#" - was too aggressive, stopped generation at inline comments
    - "\nif __name__"

# Inference Settings
inference:
  num_workers: 16  # Parallel API calls (default: 16 for I/O-bound tasks)

# Evaluation Settings
evaluation:
  timeout: 3  # Timeout per test in seconds
  num_workers: null  # null = auto-detect CPU count
  results_dir: "results"

# Dataset Settings
dataset:
  max_samples: 20  # Testing with 20 samples first, then set to null for full run

# Experiment Configurations
# Each experiment defines a unique combination of prompt strategy and settings
experiments:
  - name: "infilling_smart"
    description: "Code infilling with TODO markers (best performance)"
    enabled: false  # Disabled to test new optimized prompts
    prompt_strategy: "infilling"
    postprocess_strategy: "smart"
    temperature: 0.2
    output_file: "completions_infilling_smart.jsonl"
    results_file: "evaluation_infilling_smart.json"

  - name: "minimal_smart"
    description: "Minimal prompt with smart post-processing"
    enabled: false  # Disabled to test new optimized prompts
    prompt_strategy: "minimal"
    postprocess_strategy: "smart"
    temperature: 0.2
    output_file: "completions_minimal_smart.jsonl"
    results_file: "evaluation_minimal_smart.json"

  - name: "instructional_smart"
    description: "Instructional prompt emphasizing correctness"
    enabled: false  # Disabled to test new optimized prompts
    prompt_strategy: "instructional"
    postprocess_strategy: "smart"
    temperature: 0.1
    output_file: "completions_instructional_smart.jsonl"
    results_file: "evaluation_instructional_smart.json"

  - name: "fewshot_smart"
    description: "Few-shot prompt with example"
    enabled: false  # Set to true to run this experiment
    prompt_strategy: "fewshot"
    postprocess_strategy: "smart"
    temperature: 0.2
    output_file: "completions_fewshot_smart.jsonl"
    results_file: "evaluation_fewshot_smart.json"

  - name: "cot_smart"
    description: "Chain of thought reasoning prompt"
    enabled: false  # Set to true to run this experiment
    prompt_strategy: "cot"
    postprocess_strategy: "smart"
    temperature: 0.3
    output_file: "completions_cot_smart.jsonl"
    results_file: "evaluation_cot_smart.json"

  - name: "infilling_basic"
    description: "Infilling with basic post-processing"
    enabled: false
    prompt_strategy: "infilling"
    postprocess_strategy: "basic"
    temperature: 0.2
    output_file: "completions_infilling_basic.jsonl"
    results_file: "evaluation_infilling_basic.json"

  # Comparison: With vs Without Post-processing
  - name: "infilling_no_postprocess"
    description: "Infilling WITHOUT post-processing (raw model output)"
    enabled: false  # Enable to compare against infilling_smart
    prompt_strategy: "infilling"
    postprocess_strategy: "none"
    temperature: 0.2
    output_file: "completions_infilling_raw.jsonl"
    results_file: "evaluation_infilling_raw.json"

  - name: "minimal_no_postprocess"
    description: "Minimal WITHOUT post-processing (raw model output)"
    enabled: false  # Enable to compare against minimal_smart
    prompt_strategy: "minimal"
    postprocess_strategy: "none"
    temperature: 0.2
    output_file: "completions_minimal_raw.jsonl"
    results_file: "evaluation_minimal_raw.json"

  - name: "datadriven_smart"
    description: "Data-driven prompt based on analysis of all 164 problems"
    enabled: false  # Enable to test reverse-engineered prompt
    prompt_strategy: "datadriven"
    postprocess_strategy: "smart"
    temperature: 0.2
    output_file: "completions_datadriven_smart.jsonl"
    results_file: "evaluation_datadriven_smart.json"

  - name: "expert_smart"
    description: "Expert-engineered prompt with self-review, persona, edge cases"
    enabled: false  # Enable to test advanced prompting techniques
    prompt_strategy: "expert"
    postprocess_strategy: "smart"
    temperature: 0.2
    output_file: "completions_expert_smart.jsonl"
    results_file: "evaluation_expert_smart.json"

  # Compare: No post-processing vs Ultra post-processing
  - name: "minimal_none"
    description: "Minimal prompt with NO post-processing (raw output)"
    enabled: true
    prompt_strategy: "minimal"
    postprocess_strategy: "none"
    temperature: 0.2
    output_file: "completions_minimal_none.jsonl"
    results_file: "evaluation_minimal_none.json"

  - name: "minimal_ultra"
    description: "Minimal prompt with ULTRA post-processing (error recovery)"
    enabled: true
    prompt_strategy: "minimal"
    postprocess_strategy: "ultra"
    temperature: 0.2
    output_file: "completions_minimal_ultra.jsonl"
    results_file: "evaluation_minimal_ultra.json"
