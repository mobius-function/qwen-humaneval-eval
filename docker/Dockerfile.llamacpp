# llama.cpp server for CPU inference
# Use --platform to match host architecture (arm64 for Apple Silicon, amd64 for Intel)
FROM --platform=linux/arm64 ghcr.io/ggerganov/llama.cpp:server

# Set environment variables
ENV MODEL_NAME=Qwen/Qwen2.5-Coder-0.5B
ENV PORT=8000
ENV HOST=0.0.0.0

# Install Python and utilities for model download
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Install huggingface-hub for model download
RUN pip3 install --no-cache-dir huggingface-hub requests

# Create directory for models
RUN mkdir -p /models

WORKDIR /models

# Download GGUF model (we'll use a quantized version for CPU efficiency)
# Using a pre-converted GGUF model from Hugging Face
RUN python3 -c "from huggingface_hub import hf_hub_download; \
    hf_hub_download(repo_id='Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF', \
    filename='qwen2.5-coder-0.5b-instruct-q8_0.gguf', \
    local_dir='/models')"

# Expose the API port
EXPOSE 8000

# Set library path and run llama.cpp server with OpenAI-compatible API
ENV LD_LIBRARY_PATH=/app:$LD_LIBRARY_PATH
CMD ["/app/llama-server", \
     "-m", "/models/qwen2.5-coder-0.5b-instruct-q8_0.gguf", \
     "--host", "0.0.0.0", \
     "--port", "8000", \
     "-c", "2048", \
     "--n-gpu-layers", "0"]
